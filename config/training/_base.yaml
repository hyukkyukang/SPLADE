# @package training
defaults:
  # Select optimizer via `optimizer@training=adam|adamw` to keep weight_decay in sync.
  - /optimizer@_global_.training: adam

name: splade

max_steps: 150000
batch_size: 32
eval_batch_size: 32
grad_accumulation: 1
num_workers: 16
prefetch_factor: 4
max_padding: true

lr: 2e-5
warmup_steps: 6000
scheduler: linear
max_grad_norm: 1.0

precision: bf16-mixed
use_cpu: false
torch_compile: true
torch_compile_loss: true
# Options: default, reduce-overhead, and max-autotune.
torch_compile_mode: max-autotune
strategy: ddp
static_graph: true
num_devices: null
device_id: 0
sparse_top_k: 0
sparse_min_weight: 0.0

val_check_interval: 5000
limit_val_batches: 1.0
log_every_n_steps: 200
val_sample_size: 1000

progress_bar:
  enabled: true
  # We set low refresh rate hoping that we can make the training faster.
  # Updates per second:
  #   - 0.1 means 1 update every 10 seconds.
  #   - 0.2 means 1 update every 5 seconds. 
  #   - 1.0 means 1 update every 1 second. 
  refresh_rate: 0.1

validation_metrics:
  enabled: true
  k_list: [1, 5, 10, 50, 100]

temperature: 1.0

loss:
  type: in_batch
  multi_positive: true

distill:
  enabled: false
  weight: 1.0
  loss: mse
  teacher_score_key: teacher_scores
  fail_on_missing: true

regularization:
  query_weight: 0.0
  doc_weight: 0.0
  type: l1
  paper_faithful: true
  weight: null
  schedule_steps: 0

beir_eval:
  enabled: false
  datasets: []
  sample_size: 128
  max_docs: 50000
  split: test
  hf_cache_dir: null

wandb:
  project: ${oc.env:WANDB_PROJECT,splade}
  entity: ${oc.env:WANDB_ENTITY,null}
  name: ${oc.env:WANDB_NAME,null}
  group: ${oc.env:WANDB_GROUP,null}
  tags: []
  mode: ${oc.env:WANDB_MODE,online}
  log_model: false
  save_dir: ${log_dir}
